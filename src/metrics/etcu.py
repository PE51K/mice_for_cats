"""
Expected Tool-Calling Utility (ETCU).

Paper Section 3.2:
"We thus introduce a parameterized metric, expected tool-calling utility, 
which approximates actual utility in situations where a calibrated confidence 
score p is used to decide of whether or not to execute a specific tool call 
generated by the language model."

Paper utility normalization:
- tp = 1 (true positive: execute correct call)
- fn = 0 (false negative: don't execute correct call)  
- tn = 0 (true negative: don't execute incorrect call)
- fp = varies by risk level (false positive: execute incorrect call)

Paper Table 1 risk levels:
- High Risk: fp = -9, τ = 0.9
- Medium Risk: fp = -1, τ = 0.5
- Low Risk: fp = -1/9, τ = 0.1

Paper Equation 3:
Execute tool call iff p > τ, where τ = -fp/(1-fp)

Paper: "Higher expected tool-calling utility is better"
"""

import numpy as np
from typing import Dict


def compute_etcu(
    confidences: np.ndarray,
    labels: np.ndarray,
    threshold: float
) -> float:
    """
    Compute Expected Tool-Calling Utility at a given threshold.
    
    Paper: MBR decision rule - execute if confidence > threshold
    
    With normalized utilities (tp=1, fn=tn=0):
    - Utility = (#TP * 1) + (#FP * fp) / n
    - where fp = -τ/(1-τ) for threshold τ
    
    Args:
        confidences: Predicted confidence scores [n_samples]
        labels: Binary correctness labels [n_samples]
        threshold: Decision threshold τ (0.1 for low, 0.5 for medium, 0.9 for high risk)
    
    Returns:
        utility: Expected utility (higher is better)
    """
    n = len(labels)
    if n == 0:
        return 0.0
    
    # Decision: execute if confidence > threshold
    execute = confidences > threshold
    
    # True positives: execute correct calls
    tp_count = np.sum(execute & (labels == 1))
    
    # False positives: execute incorrect calls
    fp_count = np.sum(execute & (labels == 0))
    
    # Compute fp utility from threshold
    # From paper equation: τ = -fp/(1-fp) => fp = -τ/(1-τ)
    if threshold >= 1.0:
        fp_value = float('-inf')
    elif threshold <= 0.0:
        fp_value = 0.0
    else:
        fp_value = -threshold / (1 - threshold)
    
    # Total utility (normalized by n)
    utility = (tp_count * 1.0 + fp_count * fp_value) / n
    
    return float(utility)


def compute_etcu_auc(
    confidences: np.ndarray,
    labels: np.ndarray,
    granularity: int = 999
) -> float:
    """
    Compute Area Under ETCU Curve.
    
    Paper: "More generally, we can compute an expected value for any τ ∈ (0, 1). 
    This yields an 'expected tool-calling utility' curve (Figure 4) for a given 
    confidence estimator on a given dataset."
    
    "we approximate AUC by evaluating expected tool-calling utility 
    at each τ ∈ {0.001, 0.002, ..., 0.999}"
    
    Args:
        confidences: Predicted confidence scores [n_samples]
        labels: Binary correctness labels [n_samples]
        granularity: Number of threshold points (paper: 999)
    
    Returns:
        auc: Average utility across all thresholds
    """
    thresholds = np.linspace(0.001, 0.999, granularity)
    utilities = [compute_etcu(confidences, labels, t) for t in thresholds]
    
    # Average utility across all thresholds
    # Paper: "taking the average of the expected tool-calling utility values 
    # at every point along the curve"
    auc = np.mean(utilities)
    
    return float(auc)


def compute_all_etcu_metrics(
    confidences: np.ndarray,
    labels: np.ndarray
) -> Dict[str, float]:
    """
    Compute all ETCU metrics from paper Table 1.
    
    Returns metrics at three risk levels plus AUC:
    - Low Risk (τ=0.1): Incorrect calls have low penalty
    - Medium Risk (τ=0.5): Incorrect calls have moderate penalty
    - High Risk (τ=0.9): Incorrect calls have high penalty
    - AUC: Summary across all risk levels
    
    Args:
        confidences: Predicted confidence scores [n_samples]
        labels: Binary correctness labels [n_samples]
    
    Returns:
        Dictionary with ETCU at τ=0.1, 0.5, 0.9 and AUC
    """
    return {
        "etcu_low_risk": compute_etcu(confidences, labels, 0.1),
        "etcu_medium_risk": compute_etcu(confidences, labels, 0.5),
        "etcu_high_risk": compute_etcu(confidences, labels, 0.9),
        "etcu_auc": compute_etcu_auc(confidences, labels)
    }


def get_risk_settings() -> Dict[str, Dict[str, float]]:
    """
    Get risk level settings from paper Table 1.
    
    Returns dictionary mapping risk level names to their parameters.
    """
    return {
        "low_risk": {
            "threshold": 0.1,
            "fp": -1/9,
            "description": "Low penalty for incorrect calls"
        },
        "medium_risk": {
            "threshold": 0.5,
            "fp": -1.0,
            "description": "Incorrect call as bad as missing correct call"
        },
        "high_risk": {
            "threshold": 0.9,
            "fp": -9.0,
            "description": "High penalty for incorrect calls"
        }
    }

