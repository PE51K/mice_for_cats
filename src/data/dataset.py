"""
STE Dataset loading and splitting.

Following paper Section 4.1:
- Demonstration set: 4,520 examples for few-shot prompting
- Training set: 1,500 examples (30 from each API)
- Validation set: 750 examples (15 from each API)
- Test set: 750 examples

Paper: "We label a generated tool call as correct if and only if 
it exactly matches the one given by STE."
"""

import json
from pathlib import Path
from typing import Dict, List, Tuple, Optional
from dataclasses import dataclass

import numpy as np


@dataclass
class STEExample:
    """Single STE dataset example."""
    query: str
    api_name: str
    api_description: str
    gold_action: str
    gold_action_input: str
    
    @property
    def gold_tool_call(self) -> str:
        """
        Format expected tool call output.
        
        Paper Figure 1 shows format: "action: {action}\naction_input: {action_input}"
        """
        return f"action: {self.gold_action}\naction_input: {self.gold_action_input}"


class STEDataset:
    """
    STE (Simulated Trial-and-Error) dataset loader.
    
    Paper Section 4.1:
    "Our experiments use the simulated trial-and-error (STE) dataset (Wang et al., 2024). 
    The dataset was synthetically generated by simulating plausible tool-using scenarios 
    for a given API and using GPT3.5-turbo with execution feedback to identify 
    (presumptively) correct tool calls."
    
    "The dataset consists of English-language queries that require calling 50 distinct APIs."
    """
    
    def __init__(self, data_dir: Path, seed: int = 42):
        """
        Initialize dataset loader.
        
        Args:
            data_dir: Path to simulated-trial-and-error directory
            seed: Random seed for reproducible splits
        """
        self.data_dir = Path(data_dir)
        self.seed = seed
        self.rng = np.random.RandomState(seed)
        
        self._load_data()
    
    def _load_data(self):
        """Load raw data from JSON files."""
        # Test data is organized by API
        test_path = self.data_dir / "llama-recipes/ft_datasets/tool_test.json"
        # Training data is a flat list
        train_path = self.data_dir / "llama-recipes/ft_datasets/tool_data_train_STE_full.json"
        
        with open(test_path, 'r', encoding='utf-8') as f:
            self.test_raw = json.load(f)
        
        with open(train_path, 'r', encoding='utf-8') as f:
            self.train_raw = json.load(f)
        
        print(f"Loaded {len(self.train_raw)} training examples")
        print(f"Loaded test data with {len(self.test_raw)} APIs")
    
    def _parse_example(self, raw: Dict) -> STEExample:
        """Parse raw JSON example to STEExample dataclass."""
        # Handle both list and string formats for API name
        api_name = raw['API_name_list']
        if isinstance(api_name, list):
            api_name = api_name[0]
        
        return STEExample(
            query=raw['query'],
            api_name=api_name,
            api_description=raw.get('api_descriptions', ''),
            gold_action=raw['action'],
            gold_action_input=raw['action_input']
        )
    
    def create_splits(
        self,
        demo_size: int = 4520,
        samples_per_api_train: int = 30,
        samples_per_api_val: int = 15
    ) -> Tuple[List[STEExample], List[STEExample], List[STEExample], List[STEExample]]:
        """
        Create dataset splits following paper Section 4.1.
        
        Paper: "To train MICE, we use the rest of the STE training set, split into 
        a training set of 1500 examples (30 from each API) and a validation set of 
        750 examples (15 from each API). We then evaluate MICE on STE's test set 
        of 750 examples."
        
        Args:
            demo_size: Size of demonstration set for ICL
            samples_per_api_train: Training samples per API (paper: 30)
            samples_per_api_val: Validation samples per API (paper: 15)
        
        Returns:
            demo_set: Examples for few-shot prompting (4,520)
            train_set: Training examples for MICE (1,500)
            val_set: Validation examples for hyperparameter selection (750)
            test_set: Test examples for final evaluation (750)
        """
        # Group training data by API
        api_to_examples: Dict[str, List[STEExample]] = {}
        for item in self.train_raw:
            example = self._parse_example(item)
            if example.api_name not in api_to_examples:
                api_to_examples[example.api_name] = []
            api_to_examples[example.api_name].append(example)
        
        print(f"Found {len(api_to_examples)} unique APIs in training data")
        
        # Sample stratified splits
        train_set = []
        val_set = []
        demo_pool = []
        
        for api_name, examples in api_to_examples.items():
            # Shuffle examples for this API
            examples_copy = examples.copy()
            self.rng.shuffle(examples_copy)
            
            # Take samples for train and val (stratified by API)
            train_samples = examples_copy[:samples_per_api_train]
            val_samples = examples_copy[samples_per_api_train:samples_per_api_train + samples_per_api_val]
            
            train_set.extend(train_samples)
            val_set.extend(val_samples)
            
            # Rest goes to demo pool
            demo_pool.extend(examples_copy[samples_per_api_train + samples_per_api_val:])
        
        # Sample demonstration set from remaining
        self.rng.shuffle(demo_pool)
        demo_set = demo_pool[:demo_size]
        
        # Parse test set (organized by API in raw data)
        test_set = []
        for api_name, examples in self.test_raw.items():
            for item in examples:
                test_set.append(self._parse_example(item))
        
        print(f"Created splits: demo={len(demo_set)}, train={len(train_set)}, "
              f"val={len(val_set)}, test={len(test_set)}")
        
        return demo_set, train_set, val_set, test_set
    
    def get_api_list(self) -> List[str]:
        """Get list of all unique API names."""
        apis = set()
        for item in self.train_raw:
            api_name = item['API_name_list']
            if isinstance(api_name, list):
                api_name = api_name[0]
            apis.add(api_name)
        return sorted(list(apis))

