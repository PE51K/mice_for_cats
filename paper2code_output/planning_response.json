[{"text": " # Reproduction Plan for \"MICE for CATs: Model-Internal Confidence Estimation for Calibrating Agents with Tools\"\n\n## Key Details from the Paper's Methodology\n\n### Model-Internal Confidence Estimators (MICE)\n1. **Decoding from Intermediate Layers**: MICE decodes from each intermediate layer of the language model using logit lens (nostalgebraist, 2020). This involves using the model's layer-i encoding at the previous position to generate preliminary output strings.\n2. **BERTScore Features**: MICE computes BERTScore (Zhang et al., 2020) similarities between the generated string `y` and each `y(i)`. These features are used as inputs to the MICE model.\n3. **Raw Confidence Feature**: MICE integrates the raw confidence of the language model's generation of the tool call as a feature.\n4. **Model Architecture**: MICE uses a simple supervised classifier that predicts the correctness of the generated tool call based on the input features (BERTScores and raw confidence).\n\n### Model Architecture Details\n- **Classifier**: A logistic regression model with L2 regularization strength of 2 is used for predicting the correctness of the generated tool call.\n- **Features**: The features include BERTScore similarities and the raw confidence score.\n\n### Metrics\n1. **Expected Calibration Error (ECE)**: Used to measure the calibration of the model's confidences. The smooth variant (smECE) is preferred for its consistent estimation.\n2. **Expected Tool-Calling Utility (ETCU)**: This metric combines accuracy and calibration to evaluate the performance of a tool-calling agent using well-calibrated confidence estimates.\n\n### Experiments\n- **Datasets**: The paper uses a simulated trial-and-error (STE) dataset (Wang et al., 2024) which consists of API-related queries.\n- **LLMs**: The experiments involve three LLMs: Llama3-8B-Instruct, Llama3.1-8B-Instruct, and Llama3.2-3B-Instruct.\n- **Experimental Settings**: The LLMs are evaluated in an 8-shot in-context learning setting using greedy decoding.\n\n### Baselines\n1. **Raw Confidence**: The confidence score directly from the model's output.\n2. **Histogram Regression Estimator (HRE)**: A method that recalibrates the raw confidence score using a histogram binning approach.\n3. **Kernel Regressor (NWKR)**: Uses Nadaraya-Watson kernel regression to recalibrate the confidence scores.\n4. **MICE Models**:\n   - **MICE Logistic Regressor (MICE LR)**: A logistic regression model trained to predict the correctness of the generated tool call.\n   - **MICE Random Forest (MICE RF)**: A random forest classifier with 1000 trees and maximum depth of 20, trained to predict the correctness.\n\n## Experimental Settings\n1. **Training and Validation**: The training set consists of 1500 examples and the validation set of 750 examples.\n2. **Evaluation**: The models are evaluated on the test set of 750 examples.\n3. **Features**: The features include BERTScore similarities and the raw confidence score.\n\n## Hyperparameters and Parameters\n- **Hyperparameters for MICE Models**: Specific to the logistic regression and random forest classifiers.\n- **Features**: Detailed in the methodology section, involving BERTScore similarities and raw confidence.\n\n## Evaluation Metrics\n1. **Smoothed Expected Calibration Error (smECE)**: Measures the calibration of the model's confidences.\n2. **Expected Tool-Calling Utility (ETCU)**: Combines accuracy and calibration to evaluate tool-calling agents.\n3. **AUC-ETCU**: Average expected tool-calling utility across all risk levels.\n\n## Plan for Reproduction\n\n### Step 1: Data Preparation\n1. **Download the STE Dataset**: Retrieve the simulated trial-and-error dataset from the provided URL.\n2. **Data Splitting**: Split the dataset into training, validation, and test sets in the ratio 2:1:1.\n\n### Step 2: Feature Extraction\n1. **Logit Lens Decoding**: Implement the decoding process from each intermediate layer of the language model.\n2. **BERTScore Calculation**: Compute BERTScore similarities between the final layer output and the decoded outputs from each layer.\n3. **Raw Confidence Calculation**: Extract the raw confidence score from the model's output.\n\n### Step 3: Model Training\n1. **Baseline Models**:\n   - Train the raw confidence model.\n   - Train the HRE model.\n   - Train the NWKR model.\n2. **MICE Models**:\n   - Train the MICE LR model.\n   - Train the MICE RF model.\n\n### Step 4: Model Evaluation\n1. **Calibration Evaluation**: Compute smECE for all models.\n2. **Utility Evaluation**: Compute ETCU for all models and evaluate the expected tool-calling utility.\n3. **AUC-ETCU Evaluation**: Evaluate the overall performance using AUC-ETCU.\n\n### Step 5: Zero-Shot Generalization\n1. **New API Generalization**: Train MICE models on APIs not seen during training and evaluate their performance on the test set.\n\n### Step 6: Analysis and Insights\n1. **Feature Importance**: Analyze the importance of BERTScore features and raw confidence in the MICE models.\n2. **Sample Efficiency**: Investigate the sample efficiency of the MICE models.\n\n### Step 7: Reporting and Documentation\n1. **Documentation**: Document the entire process, including code, datasets, and results.\n2. **Reproducibility**: Ensure that all code and steps are documented in a way that allows for easy reproduction.\n\n## Conclusion\nThis plan provides a detailed roadmap for reproducing the experiments and methodologies described in the paper. It includes data preparation, feature extraction, model training, evaluation, and analysis steps. The plan ensures that each aspect of the paper's methodology is thoroughly covered, allowing for a precise replication of the results."}, {"text": " [CONTENT]\n{\n    \"Implementation approach\": \"We will implement a software system that focuses on reproducing the experiments and methodologies described in the paper. The system will be designed to handle the data preparation, model training, evaluation, and generalization to new APIs. We will use PyTorch as the deep learning framework and ensure that all components are modular and easily extendable. The system will be documented with clear APIs and will be simple yet effective in its design.\",\n    \"File list\": [\n        \"main.py\",  \n        \"dataset_loader.py\", \n        \"model.py\",  \n        \"trainer.py\",\n        \"evaluation.py\",\n        \"utils.py\" \n    ],\n    \"Data structures and interfaces\": \"\nclassDiagram\n    class Main {\n        +__init__(config: dict)\n        +run_experiment() -> None\n    }\n    class DatasetLoader {\n        +__init__(config: dict)\n        +load_data() -> Any\n    }\n    class Model {\n        +__init__(params: dict)\n        +forward(x: Tensor) -> Tensor\n    }\n    class Trainer {\n        +__init__(model: Model, config: dict)\n        +train() -> None\n    }\n    class Evaluation {\n        +__init__(model: Model, config: dict)\n        +evaluate() -> dict\n    }\n    class Utils {\n        +log_metrics(metrics: dict, log_file: str) -> None\n    }\n    Main --> DatasetLoader\n    Main --> Trainer\n    Main --> Evaluation\n    Main --> Utils\n    Trainer --> Model\n\",\n    \"Program call flow\": \"\nsequenceDiagram\n    participant M as Main\n    participant DL as DatasetLoader\n    participant MD as Model\n    participant TR as Trainer\n    participant EV as Evaluation\n    participant UT as Utils\n    M->>DL: load_data()\n    DL-->>M: return dataset\n    M->>MD: initialize model(params)\n    M->>TR: train(model, dataset)\n    TR->>MD: forward(x)\n    MD-->>TR: predictions\n    TR->>UT: log_metrics(metrics, 'train_log.txt')\n    TR-->>M: training complete\n    M->>EV: evaluate(model, dataset)\n    EV->>MD: forward(x)\n    MD-->>EV: predictions\n    EV->>UT: log_metrics(metrics, 'eval_log.txt')\n    EV-->>M: metrics\n\",\n    \"Anything UNCLEAR\": \"Need clarification on the exact dataset format and any specialized hyperparameters. The configuration parameters for the models and trainers need to be specified.\"\n}\n[/CONTENT]"}, {"text": " [CONTENT]\n{\n    \"Required packages\": [\n        \"numpy==1.21.0\",\n        \"torch==1.9.0\",\n        \"pandas==1.3.3\",\n        \"transformers==4.17.0\",\n        \"scikit-learn==0.24.2\"\n    ],\n    \"Required Other language third-party packages\": [\n        \"No third-party dependencies required\"\n    ],\n    \"Logic Analysis\": [\n        [\n            \"data_preprocessing.py\",\n            \"Handles the preprocessing of the STE dataset. This includes functions for cleaning, transforming, and feature engineering.\"\n        ],\n        [\n            \"model.py\",\n            \"Defines the transformer-based language model architecture used for tool call generation.\"\n        ],\n        [\n            \"trainer.py\",\n            \"Trains the model using the specified training loop, including loss computation and optimizer updates.\"\n        ],\n        [\n            \"evaluation.py\",\n            \"Evaluates the model's performance using the smooth expected calibration error (smECE) and expected tool-calling utility (ETCU).\"\n        ],\n        [\n            \"main.py\",\n            \"Entry point of the application. Initializes data, model, and trainer, and orchestrates the training and evaluation process.\"\n        ],\n        [\n            \"utils.py\",\n            \"Contains utility functions for logging metrics, handling configuration, and other miscellaneous tasks.\"\n        ]\n    ],\n    \"Task list\": [\n        \"data_preprocessing.py\",\n        \"model.py\",\n        \"trainer.py\",\n        \"evaluation.py\",\n        \"utils.py\",\n        \"main.py\"\n    ],\n    \"Full API spec\": \"\",\n    \"Shared Knowledge\": \"Both data_preprocessing.py and trainer.py share configuration parameters for the dataset, model, and training loop.\",\n    \"Anything UNCLEAR\": \"Clarification needed on the specific structure of the STE dataset and any additional data preprocessing steps not covered in the paper.\"\n}\n[/CONTENT]"}, {"text": " ## Code: config.yaml\n```yaml\n## config.yaml\ntraining:\n  learning_rate: 0.001\n  batch_size: 32\n  epochs: 10\nevaluation:\n  smECE_smoothing: 0.1\n  ETCU_risk_levels:\n    low: -1\n    medium: -1\n    high: -9\n```"}]