[{"role": "system", "content": "You are an expert researcher, strategic analyzer and software engineer with a deep understanding of experimental design and reproducibility in scientific research.\nYou will receive a research paper in JSON format, an overview of the plan, a design in JSON format consisting of \"Implementation approach\", \"File list\", \"Data structures and interfaces\", and \"Program call flow\", followed by a task in JSON format that includes \"Required packages\", \"Required other language third-party packages\", \"Logic Analysis\", and \"Task list\", along with a configuration file named \"config.yaml\". \n\nYour task is to conduct a comprehensive logic analysis to accurately reproduce the experiments and methodologies described in the research paper. \nThis analysis must align precisely with the paper\u2019s methodology, experimental setup, and evaluation criteria.\n\n1. Align with the Paper: Your analysis must strictly follow the methods, datasets, model configurations, hyperparameters, and experimental setups described in the paper.\n2. Be Clear and Structured: Present your analysis in a logical, well-organized, and actionable format that is easy to follow and implement.\n3. Prioritize Efficiency: Optimize the analysis for clarity and practical implementation while ensuring fidelity to the original experiments.\n4. Follow design: YOU MUST FOLLOW \"Data structures and interfaces\". DONT CHANGE ANY DESIGN. Do not use public member functions that do not exist in your design.\n5. REFER TO CONFIGURATION: Always reference settings from the config.yaml file. Do not invent or assume any values\u2014only use configurations explicitly provided.\n     \n"}, {"role": "user", "content": "## Paper\n{'paper_id': 'mice_for_cats', 'title': 'MICE for CATs: Model-Internal Confidence Estimation for Calibrating Agents with Tools', 'abstract': \"Tool-using agents that act in the world need to be both useful and safe. Well-calibrated model confidences can be used to weigh the risk versus reward of potential actions, but prior work shows that many models are poorly calibrated. Inspired by interpretability literature exploring the internals of models, we propose a novel class of model-internal confidence estimators (MICE) to better assess confidence when calling tools. MICE first decodes from each intermediate layer of the language model using logit lens (nostalgebraist, 2020) and then computes similarity scores between each layer's generation and the final output. These features are fed into a learned probabilistic classifier to assess confidence in the decoded output. On the simulated trial and error (STE) tool-calling dataset using Llama3 models, we find that MICE beats or matches the baselines on smoothed expected calibration error. Using MICE confidences to determine whether to call a tool significantly improves over strong baselines on a new metric, expected tool-calling utility. Further experiments show that MICE is sample-efficient, can generalize zero-shot to unseen APIs, and results in higher tool-calling utility in scenarios with varying risk levels.\\nOur code is open source, available at https: //github.com/microsoft/mice_for_cats.\", 'pdf_parse': {'paper_id': 'mice_for_cats', 'abstract': [{'text': \"Tool-using agents that act in the world need to be both useful and safe. Well-calibrated model confidences can be used to weigh the risk versus reward of potential actions, but prior work shows that many models are poorly calibrated. Inspired by interpretability literature exploring the internals of models, we propose a novel class of model-internal confidence estimators (MICE) to better assess confidence when calling tools. MICE first decodes from each intermediate layer of the language model using logit lens (nostalgebraist, 2020) and then computes similarity scores between each layer's generation and the final output. These features are fed into a learned probabilistic classifier to assess confidence in the decoded output. On the simulated trial and error (STE) tool-calling dataset using Llama3 models, we find that MICE beats or matches the baselines on smoothed expected calibration error. Using MICE confidences to determine whether to call a tool significantly improves over strong baselines on a new metric, expected tool-calling utility. Further experiments show that MICE is sample-efficient, can generalize zero-shot to unseen APIs, and results in higher tool-calling utility in scenarios with varying risk levels.\", 'section': 'Abstract', 'sec_num': None}, {'text': 'Our code is open source, available at https: //github.com/microsoft/mice_for_cats.', 'section': 'Abstract', 'sec_num': None}], 'body_text': [{'text': 'Language models are increasingly being used as tool-using agents, where they can generate executable API calls that can change external environments (Schick et al., 2024; Yan et al., 2024; Wang et al., 2024; Roy et al., 2024) . Sometimes the generated tool calls are relatively safe, and mistakes will have minimal impact (e.g., if \"how many grand slams has Serena Williams won?\" resulted in the incorrect tool call tennis_reference_count_grand_slams( name=\"venus williams\"), then the user would just be misinformed). But other times, incorrect tool calls can be more harmful (e.g., if \"please remove slash.txt\" resulted in the incorrect tool call cli(args=\"rm -rf /\"), then the user would lose the contents of their filesystem).', 'section': 'Introduction', 'sec_num': '1'}, {'text': \"A confidence estimator estimates the probability that another model's output is correct. A simple confidence estimator for a language model would be based on the probability that the model itself assigns to its output (i.e., the product of token probabilities) or to its output's semantic equivalence class (Zhong et al., 2023; Farquhar et al., 2024 ). Yet prior work has shown that this method can be poorly calibrated (Jiang et al., 2021; Mielke et al., 2022; Kadavath et al., 2022; Yin et al., 2023) . A probabilistic classifier is well calibrated if on an unseen test distribution, it is correct about as often as it thinks it is (Dawid, 1982; Guo et al., 2017; Desai and Durrett, 2020; Zhao et al., 2021; Hashemi et al., 2024) . For example, of those unseen examples that it predicts to be positive with \u2248 25% probability, \u2248 25% really are positive. Well-calibrated probabilities can be used to guide downstream decisions, but calibration should never be one's only engineering target, as even a highly unsure classifier may be well-calibrated (see \u00a73.1).\", 'section': 'Introduction', 'sec_num': '1'}, {'text': \"To that end, we introduce a class of modelinternal confidence estimators (MICE) and an end-to-end metric, expected tool-calling utility (ETCU), to evaluate a tool-calling agent that consults a confidence estimator to decide when to launch the predicted tool call.1 MICE extracts fea- tures by decoding from the intermediate layers of a transformer-based large language model (LLM) and computes the similarities of those generations to the output of the final layer. Based on these features and the LLM's raw confidence, it learns a model that outputs a confidence score. MICE excels on ETCU, increasingly outperforming two strong baselines as the cost of incorrect tool calls increases, without increasing calibration error.\", 'section': 'Introduction', 'sec_num': '1'}, {'text': 'This paper makes the following contributions: We propose a class of model-internal confidence estimators (MICE) that are empirically wellcalibrated on the task of assessing generated tool calls ( \u00a72). We introduce a new metric, expected tool-calling utility, that combines accuracy and calibration to better evaluate tool-calling agents ( \u00a73). Finally, we show that MICE is sample-efficient and can generalize to new tools, even in a zero-shot setting ( \u00a75).', 'section': 'Introduction', 'sec_num': '1'}, {'text': \"MICE is a simple learned probabilistic classifier whose features are derived from model-internal activations. Prior work on understanding the internals of transformer language models has shown that intermediate layers at different depths encode different types of information, and that the activation spaces at various layers of these models can be nudged to generate sequences in targeted ways (Tenney et al., 2019; Subramani et al., 2019; Subramani and Suresh, 2020; Subramani et al., 2022; Turner et al., 2023) . Decoding from the layers of a transformer language model has provided insight into the underlying mechanisms and has been used in early-exit algorithms for faster generation (nostalgebraist, 2020; Geva et al., 2022; Schuster et al., 2022; Belrose et al., 2023) . For question answering tasks, decoding from roughly the first half of the layers of the language model produces unintelligible results, but in later layers the model's predictions slowly refine into a plausible answer (Merullo et al., 2024) .\", 'section': 'Model-Internal Confidence Estimators', 'sec_num': '2'}, {'text': \"We hypothesize that features from intermediate layers' hidden states could provide useful signal for calibration. Drastic changes in the final few layers could indicate the inability for the LLM to pinpoint a tool call. As a result, we may trust a prediction that was slowly refined into an answer over the final 50% of layers more than one that drastically changed in the final few layers, even if they had the same distribution at the end.\", 'section': 'Model-Internal Confidence Estimators', 'sec_num': '2'}, {'text': \"BERTScore Features Since we hypothesize that intermediate layers' hidden states could be a useful signal for calibration beyond the confidences derived from the final layer, we decode from each layer, much as in logit lens (nostalgebraist, 2020), a model interpretation technique. We first decode the output string y at temperature 0. This is the usual way to obtain model output in a task like tool calling. Then at each layer i < \u2113, we obtain a preliminary output string y (i) of the same length by per-token argmax decoding:2 \", 'section': 'Model-Internal Confidence Estimators', 'sec_num': '2'}, {'text': 'y (i) t = argmax h (i) t-1 W out (1)', 'section': 'Model-Internal Confidence Estimators', 'sec_num': '2'}, {'text': 'where each t is a token position in y, and the row vector h', 'section': 'Model-Internal Confidence Estimators', 'sec_num': '2'}, {'text': '(i)', 'section': 'Model-Internal Confidence Estimators', 'sec_num': '2'}, {'text': \"t-1 \u2208 R d is the model's layer-i encoding at the previous position, whose product with the unembedding matrix W out \u2208 R d\u00d7|V | is a vector of logits \u2208 R |V | . Here, d is the embedding size and |V | is the vocabulary size. This results in \u2113 strings, where \u2113 is the number of layers of the model.\", 'section': 'Model-Internal Confidence Estimators', 'sec_num': '2'}, {'text': 'We then compute the BERTScore (Zhang et al., 2020) between y and each y (i) . These become the main input features to the MICE model. 3Raw Confidence Feature We also integrate the raw confidence of the language model in generating the tool call as a feature to the MICE model. We calculate this by computing the product of the probabilities of the tokens in the generated tool call. We Figure 3 : BERTScore similarities between the generated string y and the preliminary strings y (i) from earlier layers, for Llama3-8B-Instruct on the STE validation set (Wang et al., 2024) . See also Figure 8 in Appendix A.', 'section': 'Model-Internal Confidence Estimators', 'sec_num': '2'}, {'text': 'notice that including formatting tokens, which are always present in the tool call, leads to increased noise and a less accurate estimate of confidence, so we omit the tokens associated with formatting. The gray tokens in Figure 1 were omitted, while the green ones were included.', 'section': 'Model-Internal Confidence Estimators', 'sec_num': '2'}, {'text': 'Model Architecture We train a simple supervised classifier that predicts whether the generated tool call y is correct. It maps from the input features-the BERTScores and the raw confidence-to a probability of correctness (i.e., a confidence estimate). Any trainable model of this form could be used here; the specific architectures and baselines that we tried will be described in \u00a74.4.', 'section': 'Model-Internal Confidence Estimators', 'sec_num': '2'}, {'text': 'Perhaps the most widely reported calibration metric is expected calibration error ( \u00a73.1). As mentioned in the introduction, however, minimizing ECE should not be our only goal. We also introduce a utility metric, expected tool-calling utility ( \u00a73.2), to assess the performance of a simple agent that makes call/no-call decisions by using our well-calibrated confidence estimates. This metric is parameterized by the cost of false positives relative to the reward of true positives.', 'section': 'Metrics', 'sec_num': '3'}, {'text': 'Expected calibration error (ECE; Naeini et al., 2014 Naeini et al., , 2015) ) is computed by constructing a histogram binned by predicted confidence, p. The accuracy of examples within a given bin is compared to the mean predicted confidence within that bin, |acc -p|. These absolute differences are then averaged across bins, with each bin weighted by the fraction of examples in that bin.', 'section': 'Expected Calibration Error (ECE)', 'sec_num': '3.1'}, {'text': 'We use a recently improved variant of ECE, smooth ECE (smECE; B\u0142asiok and Nakkiran, 2024) , which replaces histogram binning with Nadaraya-Watson kernel regression (Nadaraya, 1964; Watson, 1964) . A reflected Gaussian kernel is used; the kernel width is determined automatically from the data, yielding a consistent estimator.', 'section': 'Expected Calibration Error (ECE)', 'sec_num': '3.1'}, {'text': 'However, ECE and smECE do not distinguish between an oracle classifier that returns p = 1.0 on correct outputs and p = 0.0 on incorrect outputs, and a maximally uninformative probabilistic classifier that always predicts the base accuracy rate. That is, if 70% of all predictions are correct, then a trivial system that gave p = 0.7 on every example would be perfectly calibrated (ECE = 0), yet mostly useless! We would prefer a system that tends to return high p on correct tool calls and low p on incorrect ones, so that we can execute the former and avoid executing the latter.', 'section': 'Expected Calibration Error (ECE)', 'sec_num': '3.1'}, {'text': \"We thus introduce a parameterized metric, expected tool-calling utility, which approximates actual utility in situations where a calibrated confidence score p is used to decide of whether or not to execute a specific tool call generated by the language model. We assume we know the expected utility for each of the four possible outcomes: tp > 0 (true positive), for when the agent executes a correctly predicted call; fp < 0 (false positive), for when the agent executes an incorrectly predicted call; tn \u2248 0 (true negative), for when the agent avoids executing an incorrect call; fn \u2248 0 (false negative), for when the agent fails to execute a correct call. tn and fn may be slightly negative to account for time wasted making the unused prediction. fp may be highly negative, e.g., if the agent erroneously deletes all of the user's documents, makes a large unintended purchase, or sends an offensive email.\", 'section': 'Expected Tool-Calling Utility (ETCU)', 'sec_num': '3.2'}, {'text': 'The exact values of tp, fp, tn, and fn will depend on the specific task that the agent must perform, and could be assigned by a domain expert or learned from data, like human preferences (Christiano et al., 2017) . 4 Once these values have been assigned, the minimum Bayes risk (MBR; Bickel and Doksum, 1977, p. 27 ) decision can be calculated; it is to execute the predicted tool call if and only if the estimated confidence p is above the threshold 4 They will also often depend on the predicted API and arguments. A more careful MBR practitioner would ideally condition on these and assign utilities to each possible pair (gold specific action, chosen specific action). Our coarser expectations {tp, fp, tn, fn} result in cruder decisions.', 'section': 'Expected Tool-Calling Utility (ETCU)', 'sec_num': '3.2'}, {'text': 'p > \u03c4 def = tn -fp (tp -fn) + (tn -fp) (2)', 'section': 'Expected Tool-Calling Utility (ETCU)', 'sec_num': '3.2'}, {'text': 'Calibration ensures that of all predicted tool calls with confidence \u2248 p, about p are correct. The decision rule (2) makes either all such calls or none of them, according to whether the expected utility per call is higher with all (p tp+(1-p) fp) or with none (p fn + (1 -p) tn). The threshold \u03c4 is high (> 0.5) if avoiding bad calls (benefit tnfp) is more important than executing good calls (benefit tpfn).', 'section': 'Expected Tool-Calling Utility (ETCU)', 'sec_num': '3.2'}, {'text': 'Normalizing: These four values can be scaled by any positive constant, and translated by any real constant, without affecting the optimal threshold or the utility (modulo that affine transform) (Gleave et al., 2021) . That is, we can choose a measurement scale for our utilities (without loss of generality) such that tp = 1 and fn = 0. Two degrees of freedom still remain (tn and fp). In most toolusing scenarios, tn will be extremely close to fn, because in both cases the immediate action by the agent is the same (do not execute) and thus has the same effect regardless of the predicted action. 5If we further assume (with loss of generality) that tn = fn = 0, the intuitive interpretation is that the agent gets 1 \"credit\" (an arbitrary utility unit) for completing its task, 0 credits for doing nothing (regardless of whether that was the best decision), and fp < 0 credits for doing something wrong. The single remaining degree of freedom fp is the risk/utility ratio, defining how costly it is to attempt and fail. In this (slightly less general) case, the MBR decision rule (2) simplifies to:', 'section': 'Expected Tool-Calling Utility (ETCU)', 'sec_num': '3.2'}, {'text': 'EQUATION', 'section': 'Expected Tool-Calling Utility (ETCU)', 'sec_num': '3.2'}, {'text': 'Settings for expected tool-calling utility: To understand how confidence estimators perform at different risk levels, we choose three different values of fp under which to measure normalized risk (Table 1). Each setting of fp determines a threshold \u03c4 that the Bayes-optimal policy will use.', 'section': 'Expected Tool-Calling Utility (ETCU)', 'sec_num': '3.2'}, {'text': 'High Risk: Tasks where executing an incorrect tool call is much worse than the reverse error. We choose fp = -9 for this setting, giving \u03c4 = 0.9.', 'section': 'Expected Tool-Calling Utility (ETCU)', 'sec_num': '3.2'}, {'text': 'Medium Risk: For these tasks, executing an incorrect tool call is as bad as executing the correct tool call is good (fp = -tp = -1), giving \u03c4 = 0.5. Low Risk: These are tasks where executing an incorrect tool call has relatively low potential downside. We choose fp = -1 9 , giving \u03c4 = 0.1. Area Under Curve (AUC): More generally, we can compute an expected value for any \u03c4 \u2208 (0, 1). This yields an \"expected tool-calling utility\" curve (Figure 4 ) for a given confidence estimator on a given dataset. Any given applied setting may only be interested in a single \u03c4 along the curve. Still, to compare estimators overall, it may be useful to consolidate the curve into a single number, summarizing an estimator\\'s performance across all risk levels. Taking inspiration from the area under the receiver operating characteristic (ROC) curve (Marcum, 1960), we take the average of the expected tool-calling utility values at every point along the curve, which can be regarded as the (signed) area under the curve (AUC). 6 Since our formulation sets tn = fn = 0, always abstaining gets a expected tool-calling utility score of 0 regardless of risk level, and thus an AUC of 0. Because utilities can be negative, AUC values can also be negative. This occurs when model is overconfident (due to poor calibration) in too many high-risk predictions.', 'section': 'Expected Tool-Calling Utility (ETCU)', 'sec_num': '3.2'}, {'text': 'We now look at training MICE and using it at test time to measure both smooth expected calibration error (smECE) and expected tool-calling utility.', 'section': 'Experiments', 'sec_num': '4'}, {'text': 'Our experiments use the simulated trial-and-error (STE) dataset (Wang et al., 2024) . The dataset was synthetically generated by simulating plausible tool-using scenarios for a given API and using GPT3.5-turbo with execution feedback to identify (presumptively) correct tool calls.', 'section': 'Dataset', 'sec_num': '4.1'}, {'text': 'The dataset consists of English-language queries that require calling 50 distinct APIs. For tool call generation, we few-shot prompt an off-the-shelf LLM with examples from a demonstration set consisting of 4,520 examples taken from the STE training set. An alternative would have been to fine-tune the LLM on this demonstration set.', 'section': 'Dataset', 'sec_num': '4.1'}, {'text': \"To train MICE, we use the rest of the STE training set, split into a training set of 1500 examples (30 from each API) and a validation set of 750 examples (15 from each API). We then evaluate MICE on STE's test set of 750 examples. In all cases, we label a generated tool call as correct if and only if it exactly matches the one given by STE.\", 'section': 'Dataset', 'sec_num': '4.1'}, {'text': 'Figure 4 : Expected tool-calling utility on the test set at varying risk levels. We include four trivial policies for reference: oracle executes only when the underlying model is correct (an upper bound); always abstain never executes, getting reward 0; always execute never abstains; and the base rate policy switches from always execute to always abstain when the risk level exceeds the base accuracy. All policies perform similarly at low risk levels, where always execute is close to optimal and hard to improve on. MICE models show clear improvements in the medium and high risk regimes.', 'section': 'Dataset', 'sec_num': '4.1'}, {'text': 'We consider three LLMs: Llama3-8B-Instruct, Llama3.1-8B-Instruct, and Llama3.2-3B-Instruct (Dubey et al., 2024) . We build and evaluate a separate MICE classifier for each LLM.', 'section': 'LLMs', 'sec_num': '4.2'}, {'text': 'We run each LLM on our validation and test sets in an 8-shot in-context learning setting, following Wang et al. (2024) , using greedy decoding to generate the tool calls y. For each evaluation example, the 8 in-context learning examples are selected from our demonstration set according to the procedure of Wang et al. (2024) , which computes similarity to the evaluation example using SentenceBERT (Reimers and Gurevych, 2019) .', 'section': 'Experimental Settings', 'sec_num': '4.3'}, {'text': 'We train a baseline or MICE regressor on the training set to predict whether tool calls are correct, and use the validation set for hyperparameter and model selection. Features used by MICE regressors were described in \u00a72. We then evaluate the regressor on the test set, using the metrics of \u00a73.', 'section': 'Experimental Settings', 'sec_num': '4.3'}, {'text': 'Raw Confidence Our first baseline is the raw confidence score from \u00a72, which can be used directly as a confidence estimate p. Recall that we defined this as i\u2208S p(w i |w <i ), where S is the subset of token indices that are relevant to the tool call. S omits the tokens associated with formatting (\"action:\" and \"action input:\", which are generated for every tool call), and also omits tokens that are generated after the arguments of the tool call. We observed in initial experiments that including these irrelevant tokens resulted in worse calibration. We also observed that taking the minimum probability across generated tokens instead of the joint probability (as in Zhou et al. (2022) ; Stengel-Eskin and Van Durme (2023a)) resulted in little effective difference. Note that calculating raw confidence does not require any learning, so neither the training nor validation set is used. Raw confidence is also used as a base feature in the estimators described below.', 'section': 'MICE Configurations & Baselines', 'sec_num': '4.4'}, {'text': \"Histogram Regression Estimator (HRE; Nobel, 1996) For our second (stronger) baseline, we use a standard method to calibrate the previous baseline. We use the training set to construct a histogram binned by raw confidence scores. We use 25 bins: [0, 0.04), [0.04, 0.08), . . . , [0.96, 1.0]. To map from a raw confidence score c to a recalibrated estimate p, we look up c's bin, and return the percentage of examples in that bin that are correct. Note that this is the same histogram construction used to calculate traditional ECE (except here constructed on the training set), and so should be expected to perform well on ECE metrics.\", 'section': 'MICE Configurations & Baselines', 'sec_num': '4.4'}, {'text': 'Kernel Regressor (NWKR) Here, rather than using a histogram with fixed bins to recalibrate, we use Nadaraya-Watson kernel regression (Nadaraya, 1964; Watson, 1964) , following the exact procedure B\u0142asiok and Nakkiran (2024) used to compute smECE. Analogously to above, since this follows the exact same procedure as in smECE, we should expect it to perform well under that metric. 7 MICE Models We extract features as described in \u00a72, using DeBERTa-xlarge-mnli to compute the BERTScore features as it is the strongest BERTScore base model (He et al., 2021) . This gives \u2113 -1 BERTScore features along with the raw confidence feature. There are \u2113 = 32 layers for Llama3 and 3.1 and 28 layers for Llama3.2.', 'section': 'MICE Configurations & Baselines', 'sec_num': '4.4'}, {'text': 'MICE Logistic Regressor (MICE LR): We train a logistic regression model with an L2 regularization strength of 2 to predict whether the tool call is correct or not.', 'section': 'MICE Configurations & Baselines', 'sec_num': '4.4'}, {'text': 'MICE Random Forest (MICE RF): We train a random forest classifier using 1000 trees each with a maximum depth of 20 and a maximum of 10 features to use at each split, using the Scikit-Learn package (Pedregosa et al., 2011) . Other hyperparameters are set to defaults. This model is also trained to predict whether the tool call is correct. 8', 'section': 'MICE Configurations & Baselines', 'sec_num': '4.4'}, {'text': 'Smooth Expected Calibration Error Lower smECE is better. The first numeric column of Table 1 shows that all of the confidence estimators are well-calibrated-their smECE values are small and not significantly different-except for the raw confidences, which have smECEs 3-10x higher than the others. This is not surprising: HRE and NWKR are explicitly designed to calibrate the raw confidences, while logistic regression and random forest training are known to produce well-calibrated classifiers (Niculescu-Mizil and Caruana, 2005) .', 'section': 'Results', 'sec_num': '5'}, {'text': 'Expected Tool-Calling Utility Figure 4 shows the expected tool-calling utility curve for each confidence estimator and each model. We find that raw confidence performs dangerously poorly at and above moderate risk levels. HRE and NWKR both degrade quickly toward 0 as risk increases. The MICE models also degrade, but more slowly: matching performance of HRE and NWKR at the lower risk levels and outperforming at medium and 7 HRE and NWKR learn to map a single confidence input feature to a recalibrated output confidence. Any confidence estimator can be calibrated in this way on held-out data. Other common approaches to this problem include isotonic regression and Platt scaling ( \u00a77).', 'section': 'Results', 'sec_num': '5'}, {'text': '8 Note that HRE and MICE LR use a similar number of parameters, but in HRE they are devoted to closer analysis (binning) of the raw confidence dimension, rather than to additional BERTScore dimensions. higher risk levels. Across all three LLMs, MICE RF performs best at nearly every risk level.', 'section': 'Results', 'sec_num': '5'}, {'text': 'Table 1 displays how well confidence estimators perform at three specific risk level settings (low, medium, and high) and across the full range of risk levels using AUC (see \u00a73.2). For all of these metrics, a higher score is better. For each risk level, MICE RF always has the highest reward, outperforming HRE, NWKR, and MICE LR. Raw token confidence nearly always performs worst. For lower risk levels, most strategies perform comparably, with relatively high reward. This is expected: executing an incorrect tool call (fp) gives a low penalty relative to a correct tool (tp), so aggressively biasing for execution is optimal, garnering a high reward. As risk levels increase, the penalty for executing an incorrect tool call grows and using raw confidences nearly always incurs a negative reward when the risk level is greater than 0.5 (fp < -1). Across the three risk levels, we find that the MICE models outperform both baselines for each of the three tool-calling LLM agents.', 'section': 'Results', 'sec_num': '5'}, {'text': 'We run permutation tests for each metric in Table 1 for each MICE method as compared to HRE and NWKR. In summary, MICE RF is always significant (p-value < 0.05) at the medium risk level, never significant at the low risk level, and significant at the high risk level for Llama3 and 3.1, but not 3.2. MICE LR outperforms the baselines, but is only significant for the medium risk level for Llama3.1. For the summary statistic AUC-ETCU, both MICE models are nearly always significantly better than HRE and NWKR for all three Llamas.', 'section': 'Results', 'sec_num': '5'}, {'text': \"To test MICE's out-of-domain generalization, we simulate encountering new APIs by holding one out during training. Since there are 50 APIs present in the STE dataset, we train 50 MICE RF and 50 MICE LR models. Each model is trained on data from 49 APIs and evaluated solely on the held-out API. We combine the predictions from each of the models to get predictions across the entire test set.9 These confidence estimates are solely constructed by MICE models that have never seen that specific API before, so every tool is unseen. MICE does worse in this setting, but only degrades to the level of HRE and NWKR models trained on the full data; they are statistically indistinguishable from them. \", 'section': 'Zero-Shot Generalization to New APIs', 'sec_num': None}, {'text': 'What is generated by decoding from intermediate layers? Here, we look at what the LLM generates from intermediary layers. Using the logit lens, we find that models slowly evolve their predictions throughout the layers to get closer to the final output generation. Figure 2 shows sample generations. Qualitatively, the first two-thirds of the layers tend to generate seemingly random strings. After this point, the generations get increasingly closer to the final generation, but significant refinement still occurs in the final layer.', 'section': 'Analysis', 'sec_num': '6'}, {'text': 'The box-and-whisker plot in Figure 3 shows that BERTScore tends to increase with layer number. Figure 8 in Appendix A shows that at some layers, the distribution of BERTScores tends to be shifted slightly higher on correct outputs, providing signal to the classifier.', 'section': 'Analysis', 'sec_num': '6'}, {'text': 'What is learned? To better understand how the MICE features are used, we examine our MICE models trained on the STE dataset with Llama3-8B-Instruct. For MICE RF we calculate Gini coefficients, and for MICE LR we analyze the feature weights, as suggested by reviewers. Figures 5 and 6 indicate that confidence is the most important feature in both MICE models: roughly 3 times as important as other features in MICE RF and 2 times as important as other features in MICE LR. 10 There is no obvious other pattern in the estimated weights, and it is possible that they are underdetermined.', 'section': 'Analysis', 'sec_num': '6'}, {'text': 'To better understand which features contribute most to confidence estimation, we performed feature ablations for both MICE models for the three LLMs in our study. In addition to the original setting with all features, we tested four new settings: confidence only; 11 first 10 Perhaps calibrated confidence would have worked even better as a feature.', 'section': 'Feature Ablations', 'sec_num': None}, {'text': \"11 For LR, this is exactly Platt scaling with L2 regularization. half of the layers' BERTScores plus confidence; second half of the layers' BERTScores plus confidence; and all of the layers' BERTScores, but no confidence. See Table 2 in Appendix A for details. MICE RF: Confidence alone performed extremely poorly. The second half of the layers plus confidence performed better than the first half plus confidence, but using all layers without confidence performed worse than using half the layers with confidence. This suggests that features from the second half of the model are more useful than the first half, and confidence is an important feature.\", 'section': 'Feature Ablations', 'sec_num': None}, {'text': \"MICE LR: Confidence alone performed comparably to other settings, indicating that confidence accounts for much of the performance; unlike RF, LR learned how to use this feature. Additionally, for Llama3 and 3.1, using confidence alone outperformed using all the layers' BERTScore features. Using the second half of the layers' BERTScore features outperformed using the first half of the layers' features, similar to MICE RF.\", 'section': 'Feature Ablations', 'sec_num': None}, {'text': 'To measure sample efficiency, we vary the size of the training set to be 25, 50, 75, 100, 300, 500, and 750. For each size, we randomly partition the 1,500 training examples into disjoint groups of that size (e.g., 15 groups of 100 examples, or 3 groups of 500 examples). We then train on each group, measure AUC-ETCU, and compute the mean and variance across groups. We repeat this 100 times and average across trials, plotting results in Figure 7 .', 'section': 'How sample efficient is MICE?', 'sec_num': None}, {'text': 'For dataset sizes of 150 or below NWKR performs best, but it saturates at this level and does not improve further with more data. MICE LR and HRE perform poorly with small datasets, but as size increases, they get closer to MICE RF and NWKR. For larger dataset sizes, MICE RF and MICE LR overtake NWKR. In fact, with as few as 300 examples (20% of the training data), MICE RF outperforms NWKR trained on the full dataset.', 'section': 'How sample efficient is MICE?', 'sec_num': None}, {'text': 'Model Internals Tenney et al. (2019) show that different layers of models encode different aspects of the classical NLP pipeline. Moreover, intermediate layer activations can be nudged via steering vectors to control output generations (Subramani et al., 2019 (Subramani et al., , 2022;; Turner et al., 2023) . The activation spaces of models are relatively well-formed and there exist directions in these latent spaces that correlate with interpretable properties (Subramani and Suresh, 2020; Li et al., 2024) . These act as part of the basis for our hypothesis that the model internals could contain a trustworthiness signal, although we did not attempt to discover specific directions in these spaces.', 'section': 'Related Work', 'sec_num': '7'}, {'text': 'We can view language models with multiple layers as doing iterative inference, where each successive layer refines the predictions of the previous layer. With this lens, decoding from intermediate layers provides signal albeit noisy: the first half of the layers generate uninterpretable text, but after this predictions refine toward a plausible answer (Belrose et al., 2023; Yom Din et al., 2024; Merullo et al., 2024) . Other work has focused on inference efficiency by early exiting from transformers (Teerapittayanon et al., 2017; Geva et al., 2022; Schuster et al., 2022; Elhoushi et al., 2024) . Our work decodes from intermediary layers as a signal for better calibration.', 'section': 'Intermediate Decoding', 'sec_num': None}, {'text': 'Calibration Prior work has measured the calibration of off-the-shelf models, including neural networks (Niculescu-Mizil and Caruana, 2005; Wang, 2024) , large language models (Kadavath et al., 2022; Yin et al., 2023) , and semantic parsers (Stengel-Eskin and Van Durme, 2023a; Zhou et al., 2022) .', 'section': 'Intermediate Decoding', 'sec_num': None}, {'text': 'A line of machine learning work focuses on calibrating binary classifiers while conditioning only on their predicted confidence. Platt scaling transforms a real-valued output (like that of an SVM classifier) into probabilities using logistic regression (Platt, 1999) , which is proven to be equivalent to beta calibration up to preprocessing (B\u00f6ken, 2021) . Isotonic regression (Ayer et al., 1955) is a non-parametric approach that learns a best fit to data making only a monotonic non-decreasing assumption. HREs are popular, and there has been work on adaptive binning strategies (Nobel, 1996) . We chose HRE and NWKR as strong baselines from this class of models. MICE LR could be viewed as an extension to Platt scaling because MICE conditions on model internals in addition to the original confidence.', 'section': 'Intermediate Decoding', 'sec_num': None}, {'text': 'The DidYouMean system can rephrase a query and ask for confirmation when the model is unconfident (Stengel-Eskin and Van Durme, 2023b). Like us, they frame the competing concerns in terms of safety and utility, weighing wrongly predicted actions against the cost of asking clarifying questions. While they tune a single confidence threshold, we transform confidences into calibrated probabilities so that a Bayes-optimal threshold can be dynamically derived for any risk/reward ratio. LA-CIE (Stengel-Eskin et al., 2024) communicates its fine-tuned confidences to users. APEL (Zhong et al., 2023) reduces its uncertainty about a semantic parse by asking questions of a user, using raw confidences to identify informative questions; calibrated confidences should work better, allowing it to finish with fewer questions.', 'section': 'Applications of Well-Calibrated Confidences', 'sec_num': None}, {'text': 'In this work, we introduce model-internal confidence estimators (MICE), which improve the trustworthiness and safety of language models as toolcalling agents. We introduce a new metric, expected tool-calling utility, that combines calibration and usefulness to better evaluate the safety and utility of tool calls. We show that MICE matches or beats both regression baselines (HRE and NWKR) when measured by smooth ECE, and significantly improves expected tool-calling utility, especially in medium and high-risk regimes. Finally, we find that MICE is sample efficient and can generalize to unseen APIs in a zero-shot setting.', 'section': 'Conclusion', 'sec_num': '8'}, {'text': 'Like logit lens, MICE assumes transformer language model whose intermediate layers have the same shape as the final layer. More generally, MICE requires access to model internals, ruling out some of the most capable current LLMs, which are closed.', 'section': 'Conclusion', 'sec_num': '8'}, {'text': 'In principle, MICE is a general-purpose confidence estimation recipe for transformer language models. However, we evaluated MICE exclusively in one setting: a tool-calling task on one dataset. Other settings such as machine translation and question answering (see footnote 1) have been left to future experiments.', 'section': 'Conclusion', 'sec_num': '8'}, {'text': 'As footnote 3 hinted, there are many other possible ways to compute MICE features. We do not claim to have found the best variant even for the setting we studied. While we settled on BERTScore for this paper, there are several other possible choices for how to encode, align, compare, and aggregate the tokens at each layer. We remark that one possible encoding trick would be to learn a linear transform of each layer i so that h (i) t-1 is maximally similar to h (\u2113) t-1 or maximally predictive of y t , as in the tuned lens of (Belrose et al., 2023) .', 'section': 'Conclusion', 'sec_num': '8'}, {'text': 'There are also various ways to build a classifier that uses MICE features. We also experimented with SVMs with different kernels (not reported). Other options could also be tried.', 'section': 'Conclusion', 'sec_num': '8'}, {'text': 'Better calibrated models can help people make safer decisions. We hope to bring increased focus to risk/reward tradeoffs; we have intentionally framed the task and metric in a way that highlights the cost of false positives. Decision theory and reward functions are not a substitute for careful design, however; practitioners must exercise great care before hooking up an LLM to a tool with real effects in the world, including taking care to set appropriate rewards such as tp, fp, tn, fn.', 'section': 'Impact Statement', 'sec_num': '10'}, {'text': 'We train and test confidence estimators specifically on the generation of tool calls-a new setting for confidence estimation. However, MICE could equally well be applied to well-studied confidence estimation settings in NLP, such as machine translation(Blatz et al.,', 'section': '', 'sec_num': None}, {'text': '2004;Kumar and Sarawagi, 2019;Wang et al., 2020), long-form generation(Band et al., 2024), and semantic parsing(Stengel-Eskin and Van Durme, 2023a).', 'section': '', 'sec_num': None}, {'text': 'Note that taking i = \u2113 in (1) would recover y. Because the transformer uses residual connections, each layerwise encoding h (i)t-1 has the same dimensionality d, so multiplication by the unembedding matrix Woutis defined even when i < \u2113. All of these vector-matrix products can be computed in parallel by a matrix-matrix product, H(i) Wout where H (i) \u2208 R \u2113\u00d7d .', 'section': '', 'sec_num': None}, {'text': '3 BERTScore reencodes the strings y and y(i) with a separate model (see \u00a74.4) and aligns their tokens. We found that the alignments were not always trivial. BERTscore performed significantly better than methods we explored initially, which compared the softmax(h(i)t-1 Wout) distributions rather than argmax-decoding single strings y(i) . See \u00a79 for other options.', 'section': '', 'sec_num': None}, {'text': 'tn might differ from fn because subsequent actions may diverge, and each utility should ideally include the expected future reward over all possible rollouts. For example, it might be slightly easier to ask clarifying questions when the original prediction was correct (implying fn > tn and raising \u03c4 ).', 'section': '', 'sec_num': None}, {'text': 'In practice, we approximate AUC by evaluating expected tool-calling utility at each \u03c4 \u2208 {0.001, 0.002, . . . , 0.999}.', 'section': '', 'sec_num': None}, {'text': 'This resembles 50-fold cross validation, where each fold is constructed solely with data from one API. However, for comparability with other methods, we evaluate on the corresponding fold in the test set, not the training set.', 'section': '', 'sec_num': None}], 'back_matter': [{'text': ' (Wang et al., 2024) . Lower smECE is better, while higher tool-calling utility is better. Bold indicates the best result in each category and underline indicates the second best result in each category. ', 'section': 'annex', 'sec_num': None}], 'ref_entries': {'FIGREF0': {'num': None, 'uris': None, 'fig_num': '1', 'text': 'Figure 1: The MICE architecture.', 'type_str': 'figure'}, 'FIGREF1': {'num': None, 'uris': None, 'fig_num': '2', 'text': 'Figure 2: Example generations from the validation set across layers of the Llama3-8B-Instruct model. Generations from early layers (5, 15) are seemingly random, but later layers (25, 31) generate thematically relevant tokens. Layer 32 is the final layer.', 'type_str': 'figure'}, 'FIGREF2': {'num': None, 'uris': None, 'fig_num': '5', 'text': 'Figure 5: Feature importance for BERTScore features and confidence on the trained MICE RF model on the STE dataset for the Llama3 LLM.', 'type_str': 'figure'}, 'FIGREF3': {'num': None, 'uris': None, 'fig_num': '6', 'text': 'Figure 6: Coefficients for the trained MICE LR model on the STE dataset for the Llama3 LLM.', 'type_str': 'figure'}, 'FIGREF4': {'num': None, 'uris': None, 'fig_num': '7', 'text': 'Figure 7: Sample complexity: AUC of MICE models and HRE baselines as the size of the training set varies on the Llama3-8B-Instruct model. Error bars are one standard deviation.', 'type_str': 'figure'}}}}\n\n-----\n\n## Overview of the plan\n # Reproduction Plan for \"MICE for CATs: Model-Internal Confidence Estimation for Calibrating Agents with Tools\"\n\n## Key Details from the Paper's Methodology\n\n### Model-Internal Confidence Estimators (MICE)\n1. **Decoding from Intermediate Layers**: MICE decodes from each intermediate layer of the language model using logit lens (nostalgebraist, 2020). This involves using the model's layer-i encoding at the previous position to generate preliminary output strings.\n2. **BERTScore Features**: MICE computes BERTScore (Zhang et al., 2020) similarities between the generated string `y` and each `y(i)`. These features are used as inputs to the MICE model.\n3. **Raw Confidence Feature**: MICE integrates the raw confidence of the language model's generation of the tool call as a feature.\n4. **Model Architecture**: MICE uses a simple supervised classifier that predicts the correctness of the generated tool call based on the input features (BERTScores and raw confidence).\n\n### Model Architecture Details\n- **Classifier**: A logistic regression model with L2 regularization strength of 2 is used for predicting the correctness of the generated tool call.\n- **Features**: The features include BERTScore similarities and the raw confidence score.\n\n### Metrics\n1. **Expected Calibration Error (ECE)**: Used to measure the calibration of the model's confidences. The smooth variant (smECE) is preferred for its consistent estimation.\n2. **Expected Tool-Calling Utility (ETCU)**: This metric combines accuracy and calibration to evaluate the performance of a tool-calling agent using well-calibrated confidence estimates.\n\n### Experiments\n- **Datasets**: The paper uses a simulated trial-and-error (STE) dataset (Wang et al., 2024) which consists of API-related queries.\n- **LLMs**: The experiments involve three LLMs: Llama3-8B-Instruct, Llama3.1-8B-Instruct, and Llama3.2-3B-Instruct.\n- **Experimental Settings**: The LLMs are evaluated in an 8-shot in-context learning setting using greedy decoding.\n\n### Baselines\n1. **Raw Confidence**: The confidence score directly from the model's output.\n2. **Histogram Regression Estimator (HRE)**: A method that recalibrates the raw confidence score using a histogram binning approach.\n3. **Kernel Regressor (NWKR)**: Uses Nadaraya-Watson kernel regression to recalibrate the confidence scores.\n4. **MICE Models**:\n   - **MICE Logistic Regressor (MICE LR)**: A logistic regression model trained to predict the correctness of the generated tool call.\n   - **MICE Random Forest (MICE RF)**: A random forest classifier with 1000 trees and maximum depth of 20, trained to predict the correctness.\n\n## Experimental Settings\n1. **Training and Validation**: The training set consists of 1500 examples and the validation set of 750 examples.\n2. **Evaluation**: The models are evaluated on the test set of 750 examples.\n3. **Features**: The features include BERTScore similarities and the raw confidence score.\n\n## Hyperparameters and Parameters\n- **Hyperparameters for MICE Models**: Specific to the logistic regression and random forest classifiers.\n- **Features**: Detailed in the methodology section, involving BERTScore similarities and raw confidence.\n\n## Evaluation Metrics\n1. **Smoothed Expected Calibration Error (smECE)**: Measures the calibration of the model's confidences.\n2. **Expected Tool-Calling Utility (ETCU)**: Combines accuracy and calibration to evaluate tool-calling agents.\n3. **AUC-ETCU**: Average expected tool-calling utility across all risk levels.\n\n## Plan for Reproduction\n\n### Step 1: Data Preparation\n1. **Download the STE Dataset**: Retrieve the simulated trial-and-error dataset from the provided URL.\n2. **Data Splitting**: Split the dataset into training, validation, and test sets in the ratio 2:1:1.\n\n### Step 2: Feature Extraction\n1. **Logit Lens Decoding**: Implement the decoding process from each intermediate layer of the language model.\n2. **BERTScore Calculation**: Compute BERTScore similarities between the final layer output and the decoded outputs from each layer.\n3. **Raw Confidence Calculation**: Extract the raw confidence score from the model's output.\n\n### Step 3: Model Training\n1. **Baseline Models**:\n   - Train the raw confidence model.\n   - Train the HRE model.\n   - Train the NWKR model.\n2. **MICE Models**:\n   - Train the MICE LR model.\n   - Train the MICE RF model.\n\n### Step 4: Model Evaluation\n1. **Calibration Evaluation**: Compute smECE for all models.\n2. **Utility Evaluation**: Compute ETCU for all models and evaluate the expected tool-calling utility.\n3. **AUC-ETCU Evaluation**: Evaluate the overall performance using AUC-ETCU.\n\n### Step 5: Zero-Shot Generalization\n1. **New API Generalization**: Train MICE models on APIs not seen during training and evaluate their performance on the test set.\n\n### Step 6: Analysis and Insights\n1. **Feature Importance**: Analyze the importance of BERTScore features and raw confidence in the MICE models.\n2. **Sample Efficiency**: Investigate the sample efficiency of the MICE models.\n\n### Step 7: Reporting and Documentation\n1. **Documentation**: Document the entire process, including code, datasets, and results.\n2. **Reproducibility**: Ensure that all code and steps are documented in a way that allows for easy reproduction.\n\n## Conclusion\nThis plan provides a detailed roadmap for reproducing the experiments and methodologies described in the paper. It includes data preparation, feature extraction, model training, evaluation, and analysis steps. The plan ensures that each aspect of the paper's methodology is thoroughly covered, allowing for a precise replication of the results.\n\n-----\n\n## Design\n [CONTENT]\n{\n    \"Implementation approach\": \"We will implement a software system that focuses on reproducing the experiments and methodologies described in the paper. The system will be designed to handle the data preparation, model training, evaluation, and generalization to new APIs. We will use PyTorch as the deep learning framework and ensure that all components are modular and easily extendable. The system will be documented with clear APIs and will be simple yet effective in its design.\",\n    \"File list\": [\n        \"main.py\",  \n        \"dataset_loader.py\", \n        \"model.py\",  \n        \"trainer.py\",\n        \"evaluation.py\",\n        \"utils.py\" \n    ],\n    \"Data structures and interfaces\": \"\nclassDiagram\n    class Main {\n        +__init__(config: dict)\n        +run_experiment() -> None\n    }\n    class DatasetLoader {\n        +__init__(config: dict)\n        +load_data() -> Any\n    }\n    class Model {\n        +__init__(params: dict)\n        +forward(x: Tensor) -> Tensor\n    }\n    class Trainer {\n        +__init__(model: Model, config: dict)\n        +train() -> None\n    }\n    class Evaluation {\n        +__init__(model: Model, config: dict)\n        +evaluate() -> dict\n    }\n    class Utils {\n        +log_metrics(metrics: dict, log_file: str) -> None\n    }\n    Main --> DatasetLoader\n    Main --> Trainer\n    Main --> Evaluation\n    Main --> Utils\n    Trainer --> Model\n\",\n    \"Program call flow\": \"\nsequenceDiagram\n    participant M as Main\n    participant DL as DatasetLoader\n    participant MD as Model\n    participant TR as Trainer\n    participant EV as Evaluation\n    participant UT as Utils\n    M->>DL: load_data()\n    DL-->>M: return dataset\n    M->>MD: initialize model(params)\n    M->>TR: train(model, dataset)\n    TR->>MD: forward(x)\n    MD-->>TR: predictions\n    TR->>UT: log_metrics(metrics, 'train_log.txt')\n    TR-->>M: training complete\n    M->>EV: evaluate(model, dataset)\n    EV->>MD: forward(x)\n    MD-->>EV: predictions\n    EV->>UT: log_metrics(metrics, 'eval_log.txt')\n    EV-->>M: metrics\n\",\n    \"Anything UNCLEAR\": \"Need clarification on the exact dataset format and any specialized hyperparameters. The configuration parameters for the models and trainers need to be specified.\"\n}\n[/CONTENT]\n\n-----\n\n## Task\n [CONTENT]\n{\n    \"Required packages\": [\n        \"numpy==1.21.0\",\n        \"torch==1.9.0\",\n        \"pandas==1.3.3\",\n        \"transformers==4.17.0\",\n        \"scikit-learn==0.24.2\"\n    ],\n    \"Required Other language third-party packages\": [\n        \"No third-party dependencies required\"\n    ],\n    \"Logic Analysis\": [\n        [\n            \"data_preprocessing.py\",\n            \"Handles the preprocessing of the STE dataset. This includes functions for cleaning, transforming, and feature engineering.\"\n        ],\n        [\n            \"model.py\",\n            \"Defines the transformer-based language model architecture used for tool call generation.\"\n        ],\n        [\n            \"trainer.py\",\n            \"Trains the model using the specified training loop, including loss computation and optimizer updates.\"\n        ],\n        [\n            \"evaluation.py\",\n            \"Evaluates the model's performance using the smooth expected calibration error (smECE) and expected tool-calling utility (ETCU).\"\n        ],\n        [\n            \"main.py\",\n            \"Entry point of the application. Initializes data, model, and trainer, and orchestrates the training and evaluation process.\"\n        ],\n        [\n            \"utils.py\",\n            \"Contains utility functions for logging metrics, handling configuration, and other miscellaneous tasks.\"\n        ]\n    ],\n    \"Task list\": [\n        \"data_preprocessing.py\",\n        \"model.py\",\n        \"trainer.py\",\n        \"evaluation.py\",\n        \"utils.py\",\n        \"main.py\"\n    ],\n    \"Full API spec\": \"\",\n    \"Shared Knowledge\": \"Both data_preprocessing.py and trainer.py share configuration parameters for the dataset, model, and training loop.\",\n    \"Anything UNCLEAR\": \"Clarification needed on the specific structure of the STE dataset and any additional data preprocessing steps not covered in the paper.\"\n}\n[/CONTENT]\n\n-----\n\n## Configuration file\n```yaml\n## config.yaml\ntraining:\n  learning_rate: 0.001\n  batch_size: 32\n  epochs: 10\nevaluation:\n  smECE_smoothing: 0.1\n  ETCU_risk_levels:\n    low: -1\n    medium: -1\n    high: -9\n```\n-----\n\n## Instruction\nConduct a Logic Analysis to assist in writing the code, based on the paper, the plan, the design, the task and the previously specified configuration file (config.yaml). \nYou DON'T need to provide the actual code yet; focus on a thorough, clear analysis.\n\nWrite the logic analysis in 'main.py', which is intended for 'Entry point of the application. Initializes data, model, and trainer, and orchestrates the training and evaluation process.'.\n\n-----\n\n## Logic Analysis: main.py"}, {"role": "assistant", "content": " ### Logic Analysis for main.py\n\nIn `main.py`, we will focus on initializing the necessary components for data handling, model setup, training, and evaluation. The primary tasks include:\n\n1. **Data Preparation**: Load and preprocess the STE dataset.\n2. **Model Initialization**: Set up the language model architecture.\n3. **Training Setup**: Define the training loop with appropriate hyperparameters.\n4. **Evaluation**: Assess the model's performance using the specified metrics.\n5. **Logging and Orchestration**: Ensure all processes are logged and coordinated.\n\n### Detailed Steps in main.py\n\n1. **Import Required Libraries**:\n   - Import necessary Python libraries such as `torch`, `pandas`, `numpy`, etc.\n   - Import custom modules for data preprocessing, model setup, training, evaluation, and utility functions.\n\n2. **Load Configuration**:\n   - Read the configuration parameters from `config.yaml` using a configuration management library like `pyyaml`.\n\n3. **Data Loading and Preprocessing**:\n   - Use `data_preprocessing.py` to load and preprocess the STE dataset.\n   - Split the dataset into training, validation, and test sets based on the configuration.\n\n4. **Model Initialization**:\n   - Initialize the language model using `model.py`.\n   - Ensure the model is set to the appropriate device (CPU/GPU) based on the configuration.\n\n5. **Training Setup**:\n   - Initialize the trainer using `trainer.py` with the model, dataset, and training parameters from the configuration.\n   - Start the training process, which includes forward and backward propagation, optimizer updates, and logging metrics using `utils.py`.\n\n6. **Evaluation**:\n   - After training, evaluate the model's performance using `evaluation.py`.\n   - Calculate the smECE and ETCU metrics based on the evaluation results and log them using `utils.py`.\n\n7. **Zero-Shot Generalization**:\n   - If applicable, train models on a subset of the dataset without seen APIs and evaluate their performance on the test set.\n\n8. **Logging and Reporting**:\n   - Use `utils.py` to log training and evaluation metrics, including model checkpoints and configuration parameters.\n   - Generate reports or visualizations if necessary for analysis.\n\n### Pseudo Code for main.py\n\n```python\nimport torch\nimport pandas as pd\nimport numpy as np\nimport configparser\nfrom data_preprocessing import load_and_preprocess_data\nfrom model import LanguageModel\nfrom trainer import Trainer\nfrom evaluation import evaluate_model\nfrom utils import log_metrics, initialize_logging\n\ndef main():\n    # Initialize logging\n    initialize_logging()\n\n    # Load configuration\n    config = configparser.ConfigParser()\n    config.read('config.yaml')\n\n    # Load and preprocess data\n    train_data, val_data, test_data = load_and_preprocess_data(config)\n\n    # Initialize the model\n    model = LanguageModel(config)\n    model = model.to(config.get('device', 'type'))  # Set device based on configuration\n\n    # Initialize the trainer\n    trainer = Trainer(model, train_data, val_data, config)\n\n    # Train the model\n    trainer.train()\n\n    # Evaluate the model\n    metrics = evaluate_model(model, test_data, config)\n\n    # Log evaluation metrics\n    log_metrics(metrics, 'evaluation_metrics.log')\n\n    # Optionally, perform zero-shot generalization\n    # (This part will depend on the specific logic and configuration)\n\n    # End of program\n    print(\"Training and evaluation complete.\")\n\nif __name__ == \"__main__\":\n    main()\n```\n\n### Key Considerations\n- **Device Configuration**: Ensure that the model and data are moved to the appropriate device (CPU/GPU) based on the configuration.\n- **Hyperparameter Management**: Manage hyperparameters such as learning rate, batch size, and number of epochs using the configuration file.\n- **Logging and Reporting**: Implement logging to track the training and evaluation progress, and ensure all results are reproducible.\n\nThis logical breakdown ensures that each aspect of the experiment is covered, from data preparation to model evaluation, in accordance with the paper's methodology and the specified configuration."}]