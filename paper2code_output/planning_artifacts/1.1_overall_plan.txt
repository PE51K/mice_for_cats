 # Reproduction Plan for "MICE for CATs: Model-Internal Confidence Estimation for Calibrating Agents with Tools"

## Key Details from the Paper's Methodology

### Model-Internal Confidence Estimators (MICE)
1. **Decoding from Intermediate Layers**: MICE decodes from each intermediate layer of the language model using logit lens (nostalgebraist, 2020). This involves using the model's layer-i encoding at the previous position to generate preliminary output strings.
2. **BERTScore Features**: MICE computes BERTScore (Zhang et al., 2020) similarities between the generated string `y` and each `y(i)`. These features are used as inputs to the MICE model.
3. **Raw Confidence Feature**: MICE integrates the raw confidence of the language model's generation of the tool call as a feature.
4. **Model Architecture**: MICE uses a simple supervised classifier that predicts the correctness of the generated tool call based on the input features (BERTScores and raw confidence).

### Model Architecture Details
- **Classifier**: A logistic regression model with L2 regularization strength of 2 is used for predicting the correctness of the generated tool call.
- **Features**: The features include BERTScore similarities and the raw confidence score.

### Metrics
1. **Expected Calibration Error (ECE)**: Used to measure the calibration of the model's confidences. The smooth variant (smECE) is preferred for its consistent estimation.
2. **Expected Tool-Calling Utility (ETCU)**: This metric combines accuracy and calibration to evaluate the performance of a tool-calling agent using well-calibrated confidence estimates.

### Experiments
- **Datasets**: The paper uses a simulated trial-and-error (STE) dataset (Wang et al., 2024) which consists of API-related queries.
- **LLMs**: The experiments involve three LLMs: Llama3-8B-Instruct, Llama3.1-8B-Instruct, and Llama3.2-3B-Instruct.
- **Experimental Settings**: The LLMs are evaluated in an 8-shot in-context learning setting using greedy decoding.

### Baselines
1. **Raw Confidence**: The confidence score directly from the model's output.
2. **Histogram Regression Estimator (HRE)**: A method that recalibrates the raw confidence score using a histogram binning approach.
3. **Kernel Regressor (NWKR)**: Uses Nadaraya-Watson kernel regression to recalibrate the confidence scores.
4. **MICE Models**:
   - **MICE Logistic Regressor (MICE LR)**: A logistic regression model trained to predict the correctness of the generated tool call.
   - **MICE Random Forest (MICE RF)**: A random forest classifier with 1000 trees and maximum depth of 20, trained to predict the correctness.

## Experimental Settings
1. **Training and Validation**: The training set consists of 1500 examples and the validation set of 750 examples.
2. **Evaluation**: The models are evaluated on the test set of 750 examples.
3. **Features**: The features include BERTScore similarities and the raw confidence score.

## Hyperparameters and Parameters
- **Hyperparameters for MICE Models**: Specific to the logistic regression and random forest classifiers.
- **Features**: Detailed in the methodology section, involving BERTScore similarities and raw confidence.

## Evaluation Metrics
1. **Smoothed Expected Calibration Error (smECE)**: Measures the calibration of the model's confidences.
2. **Expected Tool-Calling Utility (ETCU)**: Combines accuracy and calibration to evaluate tool-calling agents.
3. **AUC-ETCU**: Average expected tool-calling utility across all risk levels.

## Plan for Reproduction

### Step 1: Data Preparation
1. **Download the STE Dataset**: Retrieve the simulated trial-and-error dataset from the provided URL.
2. **Data Splitting**: Split the dataset into training, validation, and test sets in the ratio 2:1:1.

### Step 2: Feature Extraction
1. **Logit Lens Decoding**: Implement the decoding process from each intermediate layer of the language model.
2. **BERTScore Calculation**: Compute BERTScore similarities between the final layer output and the decoded outputs from each layer.
3. **Raw Confidence Calculation**: Extract the raw confidence score from the model's output.

### Step 3: Model Training
1. **Baseline Models**:
   - Train the raw confidence model.
   - Train the HRE model.
   - Train the NWKR model.
2. **MICE Models**:
   - Train the MICE LR model.
   - Train the MICE RF model.

### Step 4: Model Evaluation
1. **Calibration Evaluation**: Compute smECE for all models.
2. **Utility Evaluation**: Compute ETCU for all models and evaluate the expected tool-calling utility.
3. **AUC-ETCU Evaluation**: Evaluate the overall performance using AUC-ETCU.

### Step 5: Zero-Shot Generalization
1. **New API Generalization**: Train MICE models on APIs not seen during training and evaluate their performance on the test set.

### Step 6: Analysis and Insights
1. **Feature Importance**: Analyze the importance of BERTScore features and raw confidence in the MICE models.
2. **Sample Efficiency**: Investigate the sample efficiency of the MICE models.

### Step 7: Reporting and Documentation
1. **Documentation**: Document the entire process, including code, datasets, and results.
2. **Reproducibility**: Ensure that all code and steps are documented in a way that allows for easy reproduction.

## Conclusion
This plan provides a detailed roadmap for reproducing the experiments and methodologies described in the paper. It includes data preparation, feature extraction, model training, evaluation, and analysis steps. The plan ensures that each aspect of the paper's methodology is thoroughly covered, allowing for a precise replication of the results.